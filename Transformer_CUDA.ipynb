{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "sc4gxOF5TVJP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from kernel_lib import *\n",
        "from matrix import Matrix\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\"This\", \"is\", \"a\", \"sentence\"]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "pos_enc_sequence_len = 10\n",
        "token_dims = vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encodings=[[ 1.00000000e+00  0.00000000e+00  9.99999975e-05  0.00000000e+00]\n",
            " [ 5.40302277e-01  8.41471016e-01  5.40302281e-05  8.41471046e-05]\n",
            " [-4.16146815e-01  9.09297466e-01 -4.16146831e-05  9.09297451e-05]\n",
            " [-9.89992499e-01  1.41120002e-01 -9.89992477e-05  1.41119999e-05]\n",
            " [-6.53643668e-01 -7.56802499e-01 -6.53643656e-05 -7.56802474e-05]\n",
            " [ 2.83662170e-01 -9.58924294e-01  2.83662175e-05 -9.58924284e-05]\n",
            " [ 9.60170269e-01 -2.79415488e-01  9.60170291e-05 -2.79415490e-05]\n",
            " [ 7.53902256e-01  6.56986594e-01  7.53902277e-05  6.56986595e-05]\n",
            " [-1.45500034e-01  9.89358246e-01 -1.45500035e-05  9.89358232e-05]\n",
            " [-9.11130250e-01  4.12118495e-01 -9.11130264e-05  4.12118497e-05]]\n"
          ]
        }
      ],
      "source": [
        "pos_encodings = Matrix(pos_enc_sequence_len, token_dims, np.float32, gpu=True)\n",
        "pos_encodings.alloc_on_gpu()\n",
        "gen_pos_encodings(pos_encodings.a_gpu,\n",
        "                  np.int32(pos_encodings.num_rows),\n",
        "                  np.int32(pos_encodings.num_cols),\n",
        "                  block=(pos_encodings.num_cols, pos_encodings.num_rows, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print(f\"{pos_encodings=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = \"This is a sentence\"\n",
        "sentence_toks = [0, 1, 2, 3] # Straight forward\n",
        "word2tok = {\"This\" : 0, \"is\" : 1, \"a\" : 2, \"sentence\" : 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "DNS1PI19Tvas"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings=[[ 0.41607213  0.72918266 -0.7984399   0.81226754]\n",
            " [ 0.736365   -0.09292436  0.28980532 -0.67561984]\n",
            " [-0.0515829   0.0228521   0.47834936 -0.35547632]\n",
            " [ 0.37067556 -0.24508198  0.31422633 -0.3602407 ]]\n"
          ]
        }
      ],
      "source": [
        "embeddings = Matrix(vocab_size, token_dims, np.float32, gpu=True)\n",
        "embeddings.alloc_on_gpu()\n",
        "embeddings.init_uniform_rand(math.sqrt(6.0 / (embeddings.num_rows + embeddings.num_cols)))\n",
        "\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print(f\"{embeddings=}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: Add a function that adds two matrices but has the ability to \"scale\" down matrices based on which one is bigger, etc\n",
        "# Special \"trimmed\" matrix add...\n",
        "kernel_code = \"\"\"\n",
        "// Assumes embedding matrix has been sized such that Dim(embedding_matrix) < Dim(pos_enc)\n",
        "extern \"C\" __global__ void add_pos_enc_and_embed(float* embedding_matrix, float* pos_enc, float* output, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "    output[idx] = embedding_matrix[idx] + pos_enc[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "add_pos_enc_and_embed = mod.get_function(\"add_pos_enc_and_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embeddings_w_pos=[[ 1.4160721   0.72918266 -0.7983399   0.81226754]\n",
            " [ 1.2766674   0.74854666  0.28985935 -0.6755357 ]\n",
            " [-0.46772972  0.9321496   0.47830775 -0.3553854 ]\n",
            " [-0.61931694 -0.10396197  0.31412733 -0.36022657]]\n"
          ]
        }
      ],
      "source": [
        "embeddings_w_pos = Matrix(embeddings.num_rows, embeddings.num_cols, np.float32, gpu=True)\n",
        "embeddings_w_pos.alloc_on_gpu()\n",
        "add_pos_enc_and_embed(embeddings.a_gpu,\n",
        "                      pos_encodings.a_gpu,\n",
        "                      embeddings_w_pos.a_gpu,\n",
        "                      np.int32(embeddings.num_elements()),\n",
        "                      block=(embeddings.num_elements(), 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print(f\"{embeddings_w_pos=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 56.  62.  68.  74.]\n",
            " [152. 174. 196. 218.]\n",
            " [248. 286. 324. 362.]\n",
            " [344. 398. 452. 506.]]\n"
          ]
        }
      ],
      "source": [
        "test_a = Matrix(4,4,np.float32,gpu=True)\n",
        "test_a.alloc_on_gpu()\n",
        "test_a.init_incremental()\n",
        "\n",
        "test_b = Matrix(4,4,np.float32,gpu=True)\n",
        "test_b.alloc_on_gpu()\n",
        "test_b.init_incremental()\n",
        "\n",
        "test_c = test_a * test_b\n",
        "print(test_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initially: test_c=[[ 56.  62.  68.  74.]\n",
            " [152. 174. 196. 218.]\n",
            " [248. 286. 324. 362.]\n",
            " [344. 398. 452. 506.]]\n",
            "After scalar divide: test_c=[[ 28.  31.  34.  37.]\n",
            " [ 76.  87.  98. 109.]\n",
            " [124. 143. 162. 181.]\n",
            " [172. 199. 226. 253.]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Initially: {test_c=}\")\n",
        "test_c = test_c / 2\n",
        "print(f\"After scalar divide: {test_c=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding matrix = [vocab_size, token_dims]\n",
        "# Technically you can make swap the dimensions of this and it will still work\n",
        "# One way requires a transpose, the other doesn't\n",
        "# Weights: [3 * token_dims, token_dims]\n",
        "\n",
        "#TODO: Make a weight transpose instead for learnings... Even though a bit less efficient...\n",
        "# weights = Matrix(3 * embeddings.num_cols, embeddings.num_cols, np.float32, gpu=True)\n",
        "# weights.alloc_on_gpu()\n",
        "weights_t = Matrix(embeddings_w_pos.num_cols, 3 * embeddings_w_pos.num_cols, np.float32, gpu=True)\n",
        "weights_t.alloc_on_gpu()\n",
        "\n",
        "# QKV matrix = [vocab_size, 3 * token_dims]\n",
        "QKV = Matrix(embeddings_w_pos.num_rows, weights_t.num_cols, np.float32, gpu=True)\n",
        "QKV.alloc_on_gpu()\n",
        "\n",
        "regular_matmul(embeddings_w_pos.a_gpu,\n",
        "               weights_t.a_gpu,\n",
        "               np.int32(QKV.num_rows),\n",
        "               np.int32(QKV.num_cols),\n",
        "               np.int32(embeddings_w_pos.num_cols),\n",
        "               QKV.a_gpu,\n",
        "               block=(QKV.num_cols, QKV.num_rows, 1))\n",
        "\n",
        "b = Matrix(QKV.num_cols, 1, np.float32, gpu=True)\n",
        "b.alloc_on_gpu()\n",
        "# TODO: Fix the scale here..?\n",
        "b.init_uniform_rand(math.sqrt(6.0 / (embeddings.num_rows + embeddings.num_cols)))\n",
        "\n",
        "QKV_b = Matrix(QKV.num_rows, QKV.num_cols, np.float32, gpu=True)\n",
        "QKV_b.alloc_on_gpu()\n",
        "\n",
        "add_matrix_w_vector(QKV.a_gpu,\n",
        "                    b.a_gpu,\n",
        "                    np.int32(QKV.num_rows),\n",
        "                    np.int32(QKV.num_cols),\n",
        "                    QKV_b.a_gpu,\n",
        "                    block=(QKV.num_cols, QKV.num_rows, 1))\n",
        "\n",
        "Q = Matrix(QKV_b.num_rows, QKV_b.num_cols / 3, np.float32, gpu=True)\n",
        "Q.set_gpu_matrix(QKV_b.a_gpu, stride=QKV_b.num_cols, start_idx=(0 * QKV_b.num_cols) / 3)\n",
        "\n",
        "K = Matrix(QKV_b.num_rows, QKV_b.num_cols / 3, np.float32, gpu=True)\n",
        "K.set_gpu_matrix(QKV_b.a_gpu, stride=QKV_b.num_cols, start_idx=(1 * QKV_b.num_cols) / 3)\n",
        "\n",
        "V = Matrix(QKV_b.num_rows, QKV_b.num_cols / 3, np.float32, gpu=True)\n",
        "V.set_gpu_matrix(QKV_b.a_gpu, stride=QKV_b.num_cols, start_idx=(2 * QKV_b.num_cols) / 3)\n",
        "\n",
        "score = (Q * K) / math.sqrt(Q.num_cols)\n",
        "score_scaled = score / math.sqrt(Q.num_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_a=[[ 0.  1.  2.  3.]\n",
            " [ 4.  5.  6.  7.]\n",
            " [ 8.  9. 10. 11.]\n",
            " [12. 13. 14. 15.]]\n",
            "Before: test_output=[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            "After: test_output=[[ 6.]\n",
            " [22.]\n",
            " [38.]\n",
            " [54.]]\n"
          ]
        }
      ],
      "source": [
        "test_a = Matrix(4,4,np.float32,gpu=True)\n",
        "test_a.alloc_on_gpu()\n",
        "test_a.init_incremental()\n",
        "\n",
        "test_output = Matrix(4,1,np.float32,gpu=True)\n",
        "test_output.alloc_on_gpu()\n",
        "test_output.init_incremental()\n",
        "\n",
        "print(f\"{test_a=}\")\n",
        "print(f\"Before: {test_output=}\")\n",
        "\n",
        "matrix_row_wise_add(test_a.a_gpu,\n",
        "        np.int32(test_a.num_rows),\n",
        "        np.int32(test_a.num_cols),\n",
        "        test_output.a_gpu,\n",
        "        block=(test_a.num_cols,test_a.num_rows,1),\n",
        "        shared=test_a.num_elements() * test_a.dtype().nbytes)\n",
        "\n",
        "print(f\"After: {test_output=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score=[[248327.92 284970.56 321182.72 358128.88]\n",
            " [161808.03 185685.22 209282.33 233356.56]\n",
            " [266780.1  306150.56 345059.34 384753.84]\n",
            " [247824.56 284393.3  320532.38 357403.94]]\n",
            "score_scaled=[[124163.96  142485.28  160591.36  179064.44 ]\n",
            " [ 80904.016  92842.61  104641.164 116678.28 ]\n",
            " [133390.05  153075.28  172529.67  192376.92 ]\n",
            " [123912.28  142196.66  160266.19  178701.97 ]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"{score=}\")\n",
        "print(f\"{score_scaled=}\")\n",
        "\n",
        "score_scaled_row_sum = Matrix(score_scaled.num_rows, 1, np.float32, gpu=True)\n",
        "score_scaled_row_sum.alloc_on_gpu()\n",
        "\n",
        "matrix_row_wise_add(score_scaled.a_gpu,\n",
        "        np.int32(score_scaled.num_rows),\n",
        "        np.int32(score_scaled.num_cols),\n",
        "        score_scaled_row_sum.a_gpu,\n",
        "        block=(score_scaled.num_cols, score_scaled.num_rows, 1),\n",
        "        shared=score_scaled.num_elements() * score_scaled.dtype().nbytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score_scaled_row_sum=[[606305.  ]\n",
            " [395066.06]\n",
            " [651371.9 ]\n",
            " [605077.1 ]]\n",
            "matrix[idx]: inf vector[row]:606305.000000\n",
            "matrix[idx]: inf vector[row]:606305.000000\n",
            "matrix[idx]: inf vector[row]:606305.000000\n",
            "matrix[idx]: inf vector[row]:606305.000000\n",
            "matrix[idx]: inf vector[row]:395066.062500\n",
            "matrix[idx]: inf vector[row]:395066.062500\n",
            "matrix[idx]: inf vector[row]:395066.062500\n",
            "matrix[idx]: inf vector[row]:395066.062500\n",
            "matrix[idx]: inf vector[row]:651371.875000\n",
            "matrix[idx]: inf vector[row]:651371.875000\n",
            "matrix[idx]: inf vector[row]:651371.875000\n",
            "matrix[idx]: inf vector[row]:651371.875000\n",
            "matrix[idx]: inf vector[row]:605077.125000\n",
            "matrix[idx]: inf vector[row]:605077.125000\n",
            "matrix[idx]: inf vector[row]:605077.125000\n",
            "matrix[idx]: inf vector[row]:605077.125000\n",
            "score_scaled_softmaxed=[[151728.19  98841.55 162927.02 151421.06]\n",
            " [161808.03 185685.22 209282.33 233356.56]\n",
            " [266780.1  306150.56 345059.34 384753.84]\n",
            " [247824.56 284393.3  320532.38 357403.94]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"{score_scaled_row_sum=}\")\n",
        "\n",
        "score_scaled_softmaxed = Matrix(score_scaled.num_rows, score_scaled.num_cols, np.float32, gpu=True)\n",
        "score_scaled_softmaxed.alloc_on_gpu()\n",
        "\n",
        "softmax(score_scaled.a_gpu,\n",
        "        score_scaled_row_sum.a_gpu,\n",
        "        np.int32(score_scaled.num_rows),\n",
        "        np.int32(score_scaled.num_cols),\n",
        "        score_scaled_softmaxed.a_gpu,\n",
        "        block=(score_scaled_softmaxed.num_cols, score_scaled_softmaxed.num_rows, 1))\n",
        "\n",
        "print(f\"{score_scaled_softmaxed=}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNOSc2A+76qSEFerkYh9pBl",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
