{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLyoANJiTBe-",
        "outputId": "b2406c73-9d7c-46ab-edba-a5646d57f18d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.6)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from mako->pycuda) (3.0.2)\n",
            "Downloading pytools-2025.1.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1-cp311-cp311-linux_x86_64.whl size=660393 sha256=f38f70c8bffd2e28f8d0e84e29883053a49272571bfd5f915292c3b68b20473e\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/7e/6c/d2d1451ea6424cdc3d67b36c16fa7111eafdf2034bc3405666\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.9 pycuda-2025.1 pytools-2025.1.1\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!pip3 install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.driver as cuda\n",
        "from pycuda.compiler import SourceModule\n",
        "import pycuda.autoinit\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "sc4gxOF5TVJP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_code = \"\"\"\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "extern \"C\" __global__ void generate_random_numbers(float* numbers, int n, int seed) {\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < n) {\n",
        "    curandState state;\n",
        "    curand_init(seed, idx, 0, &state);\n",
        "    numbers[idx] = curand_uniform(&state);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\" __global__ void add_positional_encoding(float* embedding_matrix, )\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,  # This is important!\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])"
      ],
      "metadata": {
        "id": "dFsy5cCSbJK-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"This is a sentence\"\n",
        "vocab = [\"This\", \"is\", \"a\", \"sentence\"]\n",
        "sentence_toks = [0, 1, 2, 3] # Straight forward\n",
        "word2tok = {\"This\" : 0, \"is\" : 1, \"a\" : 2, \"sentence\" : 3}\n",
        "\n",
        "# Create the embedding matrix\n",
        "vocab_size = len(vocab)\n",
        "embedding_dimension = 10 # num of dimensions in each vector in the embedding matrix\n",
        "embedding_num_elements = vocab_size * embedding_dimension\n",
        "embedding_size_bytes = embedding_num_elements * np.float32().nbytes\n",
        "embedding_matrix_gpu = cuda.mem_alloc(embedding_size_bytes)\n",
        "\n",
        "generate_random_numbers = mod.get_function(\"generate_random_numbers\")\n",
        "generate_random_numbers(embedding_matrix_gpu, np.int32(embedding_num_elements), np.int32(0), block=(256, 1, 1), grid=(int(np.ceil(embedding_num_elements / 256)), 1))\n",
        "\n",
        "embedding_matrix_host = np.empty(embedding_num_elements, dtype=np.float32)\n",
        "cuda.memcpy_dtoh(embedding_matrix_host, embedding_matrix_gpu)\n",
        "\n",
        "# print(embedding_matrix_host)\n",
        "\n",
        "# We need to build a method to lookup tokens in the embedding matrix\n",
        "# Use word2tok\n",
        "\n",
        "# Create Positional Encoding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNS1PI19Tvas",
        "outputId": "eabc8685-a956-4285-b835-bbaea38c128a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.74021935 0.9209938  0.03902049 0.9689629  0.92514056 0.4463501\n",
            " 0.6673192  0.10993068 0.4702186  0.51319367 0.77617514 0.29476565\n",
            " 0.71400964 0.35850185 0.68141866 0.29201493 0.319409   0.81091344\n",
            " 0.15411183 0.44516575 0.20799614 0.6109805  0.3072757  0.4155753\n",
            " 0.23426796 0.87933475 0.64623165 0.92644703 0.5785622  0.55384874\n",
            " 0.3557058  0.722922   0.27829847 0.6191796  0.5875587  0.37504062\n",
            " 0.24048613 0.41475514 0.09369452 0.6325524 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUcUeVvLUTuz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}