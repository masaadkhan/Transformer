{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "sc4gxOF5TVJP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from kernel_lib import *\n",
        "from matrix import Matrix\n",
        "\n",
        "# import importlib\n",
        "# importlib.reload(kernel_lib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "dFsy5cCSbJK-"
      },
      "outputs": [],
      "source": [
        "kernel_code = \"\"\"\n",
        "#include <curand_kernel.h>\n",
        "#include <math.h>\n",
        "\n",
        "extern \"C\" __global__ void generate_random_numbers(float* numbers, int seed, int N) {\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < N) {\n",
        "    curandState state;\n",
        "    curand_init(seed, idx, 0, &state);\n",
        "    numbers[idx] = curand_uniform(&state);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\" __global__ void debug_func(void) {\n",
        "  printf(\"Debug print %f\\\\n\", powf(2, 1));\n",
        "}\n",
        "\n",
        "extern \"C\" __global__ void calc_positional_encoding(float* pos_enc, int num_rows, int num_cols) {\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < num_rows && col < num_cols) {\n",
        "    int idx = row * num_cols + col;\n",
        "    \n",
        "    int token_idx = row;\n",
        "    int current_dim = col;\n",
        "    int token_dims = num_cols;\n",
        "\n",
        "    pos_enc[idx] = (current_dim & 1) ?\n",
        "                    sinf(token_idx) / powf(10000, (2 * current_dim) / token_dims) :\n",
        "                    cosf(token_idx) / powf(10000, (2 * current_dim) / token_dims);\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,  # This is important!\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "debug_func = mod.get_function(\"debug_func\")\n",
        "gen_pos_encodings = mod.get_function(\"calc_positional_encoding\")\n",
        "generate_random_numbers = mod.get_function(\"generate_random_numbers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\"This\", \"is\", \"a\", \"sentence\"]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "pos_enc_seq_len = 10\n",
        "token_dims = vocab_size\n",
        "\n",
        "pos_encodings_num_elements = pos_enc_seq_len * token_dims\n",
        "pos_encodings_size_bytes = pos_encodings_num_elements * np.float32().nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encoding=[[ 1.0000e+00  0.0000e+00  1.0000e-04  0.0000e+00]\n",
            " [ 5.4030e-01  8.4147e-01  5.4030e-05  8.4147e-05]\n",
            " [-4.1615e-01  9.0930e-01 -4.1615e-05  9.0930e-05]\n",
            " [-9.8999e-01  1.4112e-01 -9.8999e-05  1.4112e-05]\n",
            " [-6.5364e-01 -7.5680e-01 -6.5364e-05 -7.5680e-05]\n",
            " [ 2.8366e-01 -9.5892e-01  2.8366e-05 -9.5892e-05]\n",
            " [ 9.6017e-01 -2.7942e-01  9.6017e-05 -2.7942e-05]\n",
            " [ 7.5390e-01  6.5699e-01  7.5390e-05  6.5699e-05]\n",
            " [-1.4550e-01  9.8936e-01 -1.4550e-05  9.8936e-05]\n",
            " [-9.1113e-01  4.1212e-01 -9.1113e-05  4.1212e-05]]\n"
          ]
        }
      ],
      "source": [
        "pos_encodings_gpu = cuda.mem_alloc(pos_encodings_size_bytes)\n",
        "init_array_w_val(pos_encodings_gpu, np.int32(123), np.int32(pos_encodings_num_elements), block=(pos_encodings_num_elements,1,1))\n",
        "gen_pos_encodings(pos_encodings_gpu, np.int32(pos_enc_seq_len), np.int32(token_dims), block=(token_dims, pos_enc_seq_len, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(pos_encodings_gpu,\n",
        "                \"pos_encoding\",\n",
        "                pos_encodings_num_elements,\n",
        "                shape=[pos_enc_seq_len, token_dims],\n",
        "                verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = \"This is a sentence\"\n",
        "sentence_toks = [0, 1, 2, 3] # Straight forward\n",
        "word2tok = {\"This\" : 0, \"is\" : 1, \"a\" : 2, \"sentence\" : 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding matrix shape : (token, vector dimensions)\n",
        "embedding_num_elements = vocab_size * token_dims\n",
        "embedding_size_bytes = embedding_num_elements * np.float32().nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "DNS1PI19Tvas"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding matrix=[[0.7402 0.921  0.039  0.969 ]\n",
            " [0.9251 0.4464 0.6673 0.1099]\n",
            " [0.4702 0.5132 0.7762 0.2948]\n",
            " [0.714  0.3585 0.6814 0.292 ]]\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix_gpu = cuda.mem_alloc(embedding_size_bytes)\n",
        "# init_array(embedding_matrix_gpu, np.int32(embedding_num_elements), block=(embedding_num_elements, 1, 1))\n",
        "generate_random_numbers(embedding_matrix_gpu, np.int32(0), np.int32(embedding_num_elements), block=(embedding_num_elements, 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(embedding_matrix_gpu,\n",
        "                \"embedding matrix\",\n",
        "                embedding_num_elements,\n",
        "                shape=[vocab_size, token_dims])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "kernel_code = \"\"\"\n",
        "// Assumes embedding matrix has been sized such that Dim(embedding_matrix) < Dim(pos_enc)\n",
        "extern \"C\" __global__ void add_pos_enc_and_embed(float* embedding_matrix, float* pos_enc, float* output, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "    output[idx] = embedding_matrix[idx] + pos_enc[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "add_pos_enc_and_embed = mod.get_function(\"add_pos_enc_and_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encoded_emb=[[ 1.7402  0.921   0.0391  0.969 ]\n",
            " [ 1.4654  1.2878  0.6674  0.11  ]\n",
            " [ 0.0541  1.4225  0.7761  0.2949]\n",
            " [-0.276   0.4996  0.6813  0.292 ]]\n"
          ]
        }
      ],
      "source": [
        "pos_encoded_emb_gpu = cuda.mem_alloc(embedding_size_bytes)\n",
        "add_pos_enc_and_embed(embedding_matrix_gpu,\n",
        "                      pos_encodings_gpu,\n",
        "                      pos_encoded_emb_gpu,\n",
        "                      np.int32(embedding_num_elements),\n",
        "                      block=(embedding_num_elements, 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(pos_encoded_emb_gpu,\n",
        "                \"pos_encoded_emb\",\n",
        "                embedding_num_elements,\n",
        "                shape=[vocab_size, token_dims])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "LUcUeVvLUTuz"
      },
      "outputs": [],
      "source": [
        "# Take the input sentence\n",
        "# convert to tokens (idices)\n",
        "# TODO(MASAAD): Do this later, assume done for now\n",
        "# Use sentence_toks\n",
        "\n",
        "# use tokens as lookup into embedding matrix\n",
        "# Add embedding element + positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_layer_code = \"\"\"\n",
        "// extern \"C\" __device__ void \n",
        "\n",
        "// Inputs x is a matrix and w is a vector\n",
        "// Dereference the vector in x and vector multiply by w\n",
        "extern \"C\" __global__ void linear_layer(float* x, float* w, int num_rows, int num_cols) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 56.  62.  68.  74.]\n",
            " [152. 174. 196. 218.]\n",
            " [248. 286. 324. 362.]\n",
            " [344. 398. 452. 506.]]\n"
          ]
        }
      ],
      "source": [
        "test_a = Matrix(4,4,np.float32,gpu=True)\n",
        "test_a.alloc_on_gpu()\n",
        "test_a.init_incremental()\n",
        "\n",
        "test_b = Matrix(4,4,np.float32,gpu=True)\n",
        "test_b.alloc_on_gpu()\n",
        "test_b.init_incremental()\n",
        "\n",
        "test_c = test_a * test_b\n",
        "print(test_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initially: test_c=[[ 56.  62.  68.  74.]\n",
            " [152. 174. 196. 218.]\n",
            " [248. 286. 324. 362.]\n",
            " [344. 398. 452. 506.]]\n",
            "After scalar divide: test_c=[[ 28.  31.  34.  37.]\n",
            " [ 76.  87.  98. 109.]\n",
            " [124. 143. 162. 181.]\n",
            " [172. 199. 226. 253.]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Initially: {test_c=}\")\n",
        "test_c = test_c / 2\n",
        "print(f\"After scalar divide: {test_c=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[185758.16  213079.12  240097.53  267631.03 ]\n",
            " [ 58527.395  67138.09   75653.87   84330.98 ]\n",
            " [ 95719.59  109798.766 123723.234 137910.02 ]\n",
            " [105880.4   121453.625 136854.58  152549.8  ]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Embedding matrix = [vocab_size, token_dims]\n",
        "# Technically you can make swap the dimensions of this and it will still work\n",
        "# One way requires a transpose, the other doesn't\n",
        "# Weights: [3 * token_dims, token_dims]\n",
        "\n",
        "weights_dim = token_dims\n",
        "weights_num_elements = 3 * token_dims * weights_dim\n",
        "weights_size_bytes = weights_num_elements * np.float32().nbytes\n",
        "weights_matrix_gpu = cuda.mem_alloc(weights_size_bytes)\n",
        "\n",
        "# QKV matrix = [vocab_size, 3 * token_dims]\n",
        "qkv_matrix_dim = vocab_size\n",
        "qkv_matrix_num_elements = qkv_matrix_dim * 3 * token_dims\n",
        "qkv_matrix_bytes = qkv_matrix_num_elements * np.float32().nbytes\n",
        "qkv_matrix_gpu = cuda.mem_alloc(qkv_matrix_bytes)\n",
        "\n",
        "regular_matmul(embedding_matrix_gpu, weights_matrix_gpu, np.int32(vocab_size), np.int32(3 * token_dims), np.int32(token_dims), qkv_matrix_gpu, block=(3 * token_dims, vocab_size, 1))\n",
        "\n",
        "bias_vector_dim = 3 * token_dims\n",
        "bias_size_bytes = bias_vector_dim * np.float32().nbytes\n",
        "bias_vector_gpu = cuda.mem_alloc(bias_size_bytes)\n",
        "\n",
        "generate_random_numbers(bias_vector_gpu, np.int32(0), np.int32(bias_vector_dim), block=(bias_vector_dim, 1, 1))\n",
        "\n",
        "qkv_matrix_w_bias_gpu = cuda.mem_alloc(qkv_matrix_bytes)\n",
        "add_matrix_w_vector(qkv_matrix_gpu, bias_vector_gpu, np.int32(qkv_matrix_dim), np.int32(3 * token_dims), qkv_matrix_w_bias_gpu, block=(3 * token_dims, qkv_matrix_dim, 1))\n",
        "\n",
        "qkv_matrix = Matrix(qkv_matrix_dim, 3 * token_dims, np.float32, gpu=True)\n",
        "qkv_matrix.set_gpu_matrix(qkv_matrix_w_bias_gpu)\n",
        "\n",
        "Q = Matrix(qkv_matrix_dim, token_dims, np.float32, gpu=True)\n",
        "Q.set_gpu_matrix(qkv_matrix_w_bias_gpu, stride=3*token_dims, start_idx=0)\n",
        "\n",
        "K = Matrix(qkv_matrix_dim, token_dims, np.float32, gpu=True)\n",
        "K.set_gpu_matrix(qkv_matrix_w_bias_gpu, stride=3*token_dims, start_idx=token_dims)\n",
        "\n",
        "V = Matrix(qkv_matrix_dim, token_dims, np.float32, gpu=True)\n",
        "V.set_gpu_matrix(qkv_matrix_w_bias_gpu, stride=3*token_dims, start_idx=2*token_dims)\n",
        "\n",
        "# mul = Q * K\n",
        "score = (Q * K) / math.sqrt(Q.num_cols)\n",
        "print(score)\n",
        "\n",
        "# print_gpu_array(qkv_matrix_gpu, \"qkv_matrix_gpu\", qkv_matrix_num_elements, shape=[qkv_matrix_dim, 3 * token_dims])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNOSc2A+76qSEFerkYh9pBl",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
