{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "sc4gxOF5TVJP"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from kernel_lib import *\n",
        "\n",
        "# import importlib\n",
        "# importlib.reload(kernel_lib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dFsy5cCSbJK-"
      },
      "outputs": [],
      "source": [
        "kernel_code = \"\"\"\n",
        "#include <curand_kernel.h>\n",
        "#include <math.h>\n",
        "\n",
        "extern \"C\" __global__ void generate_random_numbers(float* numbers, int seed, int N) {\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < N) {\n",
        "    curandState state;\n",
        "    curand_init(seed, idx, 0, &state);\n",
        "    numbers[idx] = curand_uniform(&state);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\" __global__ void debug_func(void) {\n",
        "  printf(\"Debug print %f\\\\n\", powf(2, 1));\n",
        "}\n",
        "\n",
        "// Init array with some provided value\n",
        "extern \"C\" __global__ void init_array_w_val(float* arr, int val, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "    arr[idx] = val;\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\" __global__ void calc_positional_encoding(float* pos_enc, int num_rows, int num_cols) {\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < num_rows && col < num_cols) {\n",
        "    int idx = row * num_cols + col;\n",
        "    \n",
        "    int token_idx = row;\n",
        "    int current_dim = col;\n",
        "    int token_dims = num_cols;\n",
        "\n",
        "    pos_enc[idx] = (current_dim & 1) ?\n",
        "                    sinf(token_idx) / powf(10000, (2 * current_dim) / token_dims) :\n",
        "                    cosf(token_idx) / powf(10000, (2 * current_dim) / token_dims);\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,  # This is important!\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "debug_func = mod.get_function(\"debug_func\")\n",
        "init_array_w_val = mod.get_function(\"init_array_w_val\")\n",
        "gen_pos_encodings = mod.get_function(\"calc_positional_encoding\")\n",
        "generate_random_numbers = mod.get_function(\"generate_random_numbers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\"This\", \"is\", \"a\", \"sentence\"]\n",
        "\n",
        "pos_enc_seq_len = 10\n",
        "token_dims = 10\n",
        "\n",
        "pos_encodings_num_elements = pos_enc_seq_len * token_dims\n",
        "pos_encodings_size_bytes = pos_encodings_num_elements * np.float32().nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encoding=[[ 1.0000e+00  0.0000e+00  1.0000e+00  0.0000e+00  1.0000e+00  0.0000e+00  1.0000e-04  0.0000e+00  1.0000e-04\n",
            "   0.0000e+00]\n",
            " [ 5.4030e-01  8.4147e-01  5.4030e-01  8.4147e-01  5.4030e-01  8.4147e-05  5.4030e-05  8.4147e-05  5.4030e-05\n",
            "   8.4147e-05]\n",
            " [-4.1615e-01  9.0930e-01 -4.1615e-01  9.0930e-01 -4.1615e-01  9.0930e-05 -4.1615e-05  9.0930e-05 -4.1615e-05\n",
            "   9.0930e-05]\n",
            " [-9.8999e-01  1.4112e-01 -9.8999e-01  1.4112e-01 -9.8999e-01  1.4112e-05 -9.8999e-05  1.4112e-05 -9.8999e-05\n",
            "   1.4112e-05]\n",
            " [-6.5364e-01 -7.5680e-01 -6.5364e-01 -7.5680e-01 -6.5364e-01 -7.5680e-05 -6.5364e-05 -7.5680e-05 -6.5364e-05\n",
            "  -7.5680e-05]\n",
            " [ 2.8366e-01 -9.5892e-01  2.8366e-01 -9.5892e-01  2.8366e-01 -9.5892e-05  2.8366e-05 -9.5892e-05  2.8366e-05\n",
            "  -9.5892e-05]\n",
            " [ 9.6017e-01 -2.7942e-01  9.6017e-01 -2.7942e-01  9.6017e-01 -2.7942e-05  9.6017e-05 -2.7942e-05  9.6017e-05\n",
            "  -2.7942e-05]\n",
            " [ 7.5390e-01  6.5699e-01  7.5390e-01  6.5699e-01  7.5390e-01  6.5699e-05  7.5390e-05  6.5699e-05  7.5390e-05\n",
            "   6.5699e-05]\n",
            " [-1.4550e-01  9.8936e-01 -1.4550e-01  9.8936e-01 -1.4550e-01  9.8936e-05 -1.4550e-05  9.8936e-05 -1.4550e-05\n",
            "   9.8936e-05]\n",
            " [-9.1113e-01  4.1212e-01 -9.1113e-01  4.1212e-01 -9.1113e-01  4.1212e-05 -9.1113e-05  4.1212e-05 -9.1113e-05\n",
            "   4.1212e-05]]\n"
          ]
        }
      ],
      "source": [
        "pos_encodings_gpu = cuda.mem_alloc(pos_encodings_size_bytes)\n",
        "init_array_w_val(pos_encodings_gpu, np.int32(123), np.int32(pos_encodings_num_elements), block=(pos_encodings_num_elements,1,1))\n",
        "gen_pos_encodings(pos_encodings_gpu, np.int32(pos_enc_seq_len), np.int32(token_dims), block=(token_dims, pos_enc_seq_len, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(pos_encodings_gpu,\n",
        "                \"pos_encoding\",\n",
        "                pos_encodings_num_elements,\n",
        "                shape=[pos_enc_seq_len, token_dims],\n",
        "                verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = \"This is a sentence\"\n",
        "sentence_toks = [0, 1, 2, 3] # Straight forward\n",
        "word2tok = {\"This\" : 0, \"is\" : 1, \"a\" : 2, \"sentence\" : 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding matrix shape : (token, vector dimensions)\n",
        "vocab_size = len(vocab)\n",
        "embedding_num_elements = vocab_size * token_dims\n",
        "embedding_size_bytes = embedding_num_elements * np.float32().nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DNS1PI19Tvas"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding matrix=[[0.7402 0.921  0.039  0.969  0.9251 0.4464 0.6673 0.1099 0.4702 0.5132]\n",
            " [0.7762 0.2948 0.714  0.3585 0.6814 0.292  0.3194 0.8109 0.1541 0.4452]\n",
            " [0.208  0.611  0.3073 0.4156 0.2343 0.8793 0.6462 0.9264 0.5786 0.5538]\n",
            " [0.3557 0.7229 0.2783 0.6192 0.5876 0.375  0.2405 0.4148 0.0937 0.6326]]\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix_gpu = cuda.mem_alloc(embedding_size_bytes)\n",
        "# init_array(embedding_matrix_gpu, np.int32(embedding_num_elements), block=(embedding_num_elements, 1, 1))\n",
        "generate_random_numbers(embedding_matrix_gpu, np.int32(0), np.int32(embedding_num_elements), block=(embedding_num_elements, 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(embedding_matrix_gpu,\n",
        "                \"embedding matrix\",\n",
        "                embedding_num_elements,\n",
        "                shape=[vocab_size, token_dims])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "kernel_code = \"\"\"\n",
        "// Assumes embedding matrix has been sized such that Dim(embedding_matrix) < Dim(pos_enc)\n",
        "extern \"C\" __global__ void add_pos_enc_and_embed(float* embedding_matrix, float* pos_enc, float* output, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "    output[idx] = embedding_matrix[idx] + pos_enc[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "add_pos_enc_and_embed = mod.get_function(\"add_pos_enc_and_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encoded_emb=[[ 1.7402  0.921   1.039   0.969   1.9251  0.4464  0.6674  0.1099  0.4703  0.5132]\n",
            " [ 1.3165  1.1362  1.2543  1.2     1.2217  0.2921  0.3195  0.811   0.1542  0.4452]\n",
            " [-0.2082  1.5203 -0.1089  1.3249 -0.1819  0.8794  0.6462  0.9265  0.5785  0.5539]\n",
            " [-0.6343  0.864  -0.7117  0.7603 -0.4024  0.3751  0.2404  0.4148  0.0936  0.6326]]\n"
          ]
        }
      ],
      "source": [
        "pos_encoded_emb_gpu = cuda.mem_alloc(embedding_size_bytes)\n",
        "add_pos_enc_and_embed(embedding_matrix_gpu,\n",
        "                      pos_encodings_gpu,\n",
        "                      pos_encoded_emb_gpu,\n",
        "                      np.int32(embedding_num_elements),\n",
        "                      block=(embedding_num_elements, 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(pos_encoded_emb_gpu,\n",
        "                \"pos_encoded_emb\",\n",
        "                embedding_num_elements,\n",
        "                shape=[vocab_size, token_dims])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "LUcUeVvLUTuz"
      },
      "outputs": [],
      "source": [
        "# Take the input sentence\n",
        "# convert to tokens (idices)\n",
        "# TODO(MASAAD): Do this later, assume done for now\n",
        "# Use sentence_toks\n",
        "\n",
        "# use tokens as lookup into embedding matrix\n",
        "# Add embedding element + positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_layer_code = \"\"\"\n",
        "// extern \"C\" __device__ void \n",
        "\n",
        "// Inputs x is a matrix and w is a vector\n",
        "// Dereference the vector in x and vector multiply by w\n",
        "extern \"C\" __global__ void linear_layer(float* x, float* w, int num_rows, int num_cols) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c_h=array([[  20.,   23.,   26.,   29.],\n",
            "       [  68.,   83.,   98.,  113.],\n",
            "       [ 116.,  143.,  170.,  197.],\n",
            "       [3784., 4630., 5476., 6322.]], dtype=float32)\n",
            "expected_result=array([[ 20.,  23.,  26.,  29.],\n",
            "       [ 56.,  68.,  80.,  92.],\n",
            "       [ 92., 113., 134., 155.],\n",
            "       [128., 158., 188., 218.]], dtype=float32)\n",
            "Matmul did not match GPU...\n"
          ]
        }
      ],
      "source": [
        "a_rows = 4\n",
        "a_cols = 3\n",
        "\n",
        "b_rows = 3\n",
        "b_cols = 4\n",
        "\n",
        "c_rows = 4\n",
        "c_cols = 4\n",
        "\n",
        "dummy_a = cuda.mem_alloc(a_rows * a_cols * 4)\n",
        "dummy_b = cuda.mem_alloc(b_rows * b_cols * 4)\n",
        "dummy_c = cuda.mem_alloc(c_rows * c_cols * 4)\n",
        "\n",
        "init_array(dummy_a, np.int32(a_rows * a_cols), block=(a_rows * a_cols,1,1))\n",
        "init_array(dummy_b, np.int32(b_rows * b_cols), block=(b_rows * b_cols,1,1))\n",
        "init_array(dummy_c, np.int32(c_rows * c_cols), block=(c_rows * c_cols,1,1))\n",
        "\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "host_a = np.empty(a_rows * a_cols, np.float32)\n",
        "host_b = np.empty(b_rows * b_cols, np.float32)\n",
        "host_c = np.empty(c_rows * c_cols, np.float32)\n",
        "\n",
        "cuda.memcpy_dtoh(host_a, dummy_a)\n",
        "cuda.memcpy_dtoh(host_b, dummy_b)\n",
        "\n",
        "regular_matmul(dummy_a, dummy_b, np.int32(c_rows), np.int32(c_cols), np.int32(a_cols), dummy_c, block=(c_rows,c_cols,1))\n",
        "cuda.Context.synchronize()\n",
        "cuda.memcpy_dtoh(host_c, dummy_c)\n",
        "\n",
        "# print_gpu_array(dummy_c, \"dummy_c\", c_rows * c_cols, shape=[c_rows,c_cols])\n",
        "\n",
        "a_h = host_a.reshape(a_rows,a_cols)\n",
        "b_h = host_b.reshape(b_rows,b_cols)\n",
        "c_h = host_c.reshape(c_rows,c_cols)\n",
        "\n",
        "print(f\"{c_h=}\")\n",
        "print(f\"{c_h=}\")\n",
        "\n",
        "expected_result = a_h @ b_h\n",
        "\n",
        "print(f\"{c_h=}\")\n",
        "print(f\"{expected_result=}\")\n",
        "\n",
        "if (np.allclose(c_h, expected_result)):\n",
        "  print(\"Matmul successful\")\n",
        "else:\n",
        "  print(\"Matmul did not match GPU...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "must specify block size",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m qkv_matrix_bytes \u001b[38;5;241m=\u001b[39m qkv_matrix_num_elements \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32()\u001b[38;5;241m.\u001b[39mnbytes\n\u001b[1;32m     14\u001b[0m qkv_matrix_gpu \u001b[38;5;241m=\u001b[39m cuda\u001b[38;5;241m.\u001b[39mmem_alloc(qkv_matrix_bytes)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mregular_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_matrix_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_matrix_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycuda/driver.py:479\u001b[0m, in \u001b[0;36m_add_functionality.<locals>.function_call\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra keyword arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(kwargs\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m    476\u001b[0m     )\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust specify block size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m func\u001b[38;5;241m.\u001b[39m_set_block_shape(\u001b[38;5;241m*\u001b[39mblock)\n\u001b[1;32m    482\u001b[0m handlers, arg_buf \u001b[38;5;241m=\u001b[39m _build_arg_buf(args)\n",
            "\u001b[0;31mValueError\u001b[0m: must specify block size"
          ]
        }
      ],
      "source": [
        "# Embedding matrix = [vocab_len, token_dims]\n",
        "# Technically you can make swap the dimensions of this and it will still work\n",
        "# One way requires a transpose, the other doesn't\n",
        "# Weights: [3 * token_dims, token_dims]\n",
        "weights_dim = token_dims\n",
        "weights_num_elements = 3 * token_dims * weights_dim\n",
        "weights_size_bytes = weights_num_elements * np.float32().nbytes\n",
        "weights_matrix_gpu = cuda.mem_alloc(weights_size_bytes)\n",
        "\n",
        "# QKV matrix = [vocab_len, 3 * token_dims]\n",
        "qkv_matrix_dim = vocab_size\n",
        "qkv_matrix_num_elements = qkv_matrix_dim * 3 * token_dims\n",
        "qkv_matrix_bytes = qkv_matrix_num_elements * np.float32().nbytes\n",
        "qkv_matrix_gpu = cuda.mem_alloc(qkv_matrix_bytes)\n",
        "\n",
        "regular_matmul(embedding_matrix_gpu, weights_matrix_gpu, np.int32(), np.int32())\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNOSc2A+76qSEFerkYh9pBl",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
