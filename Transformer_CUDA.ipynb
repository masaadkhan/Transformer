{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sc4gxOF5TVJP"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import kernel_lib as kl\n",
        "import pycuda.driver as cuda\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np\n",
        "from matrix import Matrix\n",
        "import math\n",
        "\n",
        "kl.compile_kernels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\"This\", \"is\", \"a\", \"sentence\"]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "pos_enc_sequence_len = 10\n",
        "token_dims = vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 40 on host to print this matrix!\n",
            "Allocating 40 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "[[ 0.          1.          0.          1.        ]\n",
            " [ 0.841471    0.5403023   0.00999983  0.99995   ]\n",
            " [ 0.90929747 -0.4161468   0.01999867  0.9998    ]\n",
            " [ 0.14112    -0.9899925   0.0299955   0.99955004]\n",
            " [-0.7568025  -0.65364367  0.03998933  0.9992001 ]\n",
            " [-0.9589243   0.28366217  0.04997917  0.99875027]\n",
            " [-0.2794155   0.96017027  0.059964    0.99820054]\n",
            " [ 0.6569866   0.75390226  0.06994285  0.997551  ]\n",
            " [ 0.98935825 -0.14550003  0.07991469  0.99680173]\n",
            " [ 0.4121185  -0.91113025  0.08987855  0.9959527 ]]\n"
          ]
        }
      ],
      "source": [
        "pos_encodings = Matrix(pos_enc_sequence_len, token_dims, np.float32, gpu=True)\n",
        "pos_encodings.alloc_on_gpu()\n",
        "kl.gen_pos_encodings(pos_encodings.a_gpu,\n",
        "                  np.int32(pos_encodings.num_rows),\n",
        "                  np.int32(pos_encodings.num_cols),\n",
        "                  block=(pos_encodings.num_cols, pos_encodings.num_rows, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print(pos_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-allocated on host...\n",
            "They are similar!\n"
          ]
        }
      ],
      "source": [
        "rows_np = np.arange(pos_enc_sequence_len)\n",
        "rows_np = rows_np[:, np.newaxis]\n",
        "\n",
        "pos_cols_np = np.arange(token_dims)\n",
        "pos_cols_np = pos_cols_np[np.newaxis, :]\n",
        "pos_cols_np = np.power(10000, (2 * (pos_cols_np // 2)) / token_dims)\n",
        "\n",
        "pos_enc_pre_sin = rows_np / pos_cols_np\n",
        "\n",
        "pos_encoding_np = np.zeros(pos_enc_pre_sin.shape)\n",
        "pos_encoding_np[:, 0::2] = np.sin(pos_enc_pre_sin[:, 0::2])\n",
        "pos_encoding_np[:, 1::2] = np.cos(pos_enc_pre_sin[:, 1::2])\n",
        "\n",
        "if (pos_encodings.compare(pos_encoding_np)):\n",
        "  print(\"They are similar!\")\n",
        "else:\n",
        "  print(\"They are not similar!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = \"This is a sentence\"\n",
        "sentence_toks = [0, 1, 2, 3] # Straight forward\n",
        "word2tok = {\"This\" : 0, \"is\" : 1, \"a\" : 2, \"sentence\" : 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DNS1PI19Tvas"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "embeddings=[[ 0.41607213  0.72918266 -0.7984399   0.81226754]\n",
            " [ 0.736365   -0.09292436  0.28980532 -0.67561984]\n",
            " [-0.0515829   0.0228521   0.47834936 -0.35547632]\n",
            " [ 0.37067556 -0.24508198  0.31422633 -0.3602407 ]]\n"
          ]
        }
      ],
      "source": [
        "embeddings = Matrix(vocab_size, token_dims, np.float32, gpu=True)\n",
        "embeddings.alloc_on_gpu()\n",
        "embeddings_scale = kl.xavier_uniform(embeddings.num_rows, embeddings.num_cols)\n",
        "embeddings.init_uniform_rand(embeddings_scale)\n",
        "\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print(f\"{embeddings=}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-allocated on host...\n",
            "[[ 0.41607213  0.72918266 -0.7984399   0.81226754]\n",
            " [ 0.736365   -0.09292436  0.28980532 -0.67561984]\n",
            " [-0.0515829   0.0228521   0.47834936 -0.35547632]\n",
            " [ 0.37067556 -0.24508198  0.31422633 -0.3602407 ]]\n"
          ]
        }
      ],
      "source": [
        "embeddings.copy_d_to_h()\n",
        "embeddings_np = embeddings.a_host.copy()\n",
        "print(embeddings_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: Add a function that adds two matrices but has the ability to \"scale\" down matrices based on which one is bigger, etc\n",
        "# Special \"trimmed\" matrix add...\n",
        "kernel_code = \"\"\"\n",
        "// Assumes embedding matrix has been sized such that Dim(embedding_matrix) < Dim(pos_enc)\n",
        "extern \"C\" __global__ void add_pos_enc_and_embed(float* embedding_matrix, float* pos_enc, float* output, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "    output[idx] = embedding_matrix[idx] + pos_enc[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "add_pos_enc_and_embed = mod.get_function(\"add_pos_enc_and_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "embeddings_w_pos=[[ 0.41607213  1.7291827  -0.7984399   1.8122675 ]\n",
            " [ 1.577836    0.44737792  0.29980516  0.32433015]\n",
            " [ 0.8577146  -0.39329472  0.49834803  0.6443237 ]\n",
            " [ 0.5117956  -1.2350745   0.34422183  0.63930935]]\n"
          ]
        }
      ],
      "source": [
        "embeddings_w_pos = Matrix(embeddings.num_rows, embeddings.num_cols, np.float32, gpu=True)\n",
        "embeddings_w_pos.alloc_on_gpu()\n",
        "add_pos_enc_and_embed(embeddings.a_gpu,\n",
        "                      pos_encodings.a_gpu,\n",
        "                      embeddings_w_pos.a_gpu,\n",
        "                      np.int32(embeddings.num_elements()),\n",
        "                      block=(embeddings.num_elements(), 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print(f\"{embeddings_w_pos=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-allocated on host...\n",
            "Embeddings are similar!\n"
          ]
        }
      ],
      "source": [
        "trimmed_pos_enc_np = pos_encoding_np[:4, :]\n",
        "embeddings_w_pos_np = embeddings_np + trimmed_pos_enc_np\n",
        "\n",
        "if (embeddings_w_pos.compare(embeddings_w_pos_np)):\n",
        "  print(\"Embeddings are similar!\")\n",
        "else:\n",
        "  print(f\"{embeddings_np=}\")\n",
        "  print(f\"{trimmed_pos_enc_np=}\")\n",
        "  print(f\"{embeddings_w_pos_np=}\")\n",
        "  print(\"Embeddings not similar!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "[[ 56.  62.  68.  74.]\n",
            " [152. 174. 196. 218.]\n",
            " [248. 286. 324. 362.]\n",
            " [344. 398. 452. 506.]]\n"
          ]
        }
      ],
      "source": [
        "test_a = Matrix(4,4,np.float32,gpu=True)\n",
        "test_a.alloc_on_gpu()\n",
        "test_a.init_incremental()\n",
        "\n",
        "test_b = Matrix(4,4,np.float32,gpu=True)\n",
        "test_b.alloc_on_gpu()\n",
        "test_b.init_incremental()\n",
        "\n",
        "test_c = test_a * test_b\n",
        "print(test_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Returning the cached matrix value!\n",
            "Initially: test_c=[[ 56.  62.  68.  74.]\n",
            " [152. 174. 196. 218.]\n",
            " [248. 286. 324. 362.]\n",
            " [344. 398. 452. 506.]]\n",
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "After scalar divide: test_c=[[ 28.  31.  34.  37.]\n",
            " [ 76.  87.  98. 109.]\n",
            " [124. 143. 162. 181.]\n",
            " [172. 199. 226. 253.]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Initially: {test_c=}\")\n",
        "test_c = test_c / 2\n",
        "print(f\"After scalar divide: {test_c=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 48 on host to print this matrix!\n",
            "Allocating 48 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n"
          ]
        },
        {
          "ename": "LogicError",
          "evalue": "cuMemcpyDtoH failed: invalid argument",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLogicError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 33\u001b[0m\n\u001b[1;32m     24\u001b[0m QKV_b\u001b[38;5;241m.\u001b[39malloc_on_gpu()\n\u001b[1;32m     26\u001b[0m kl\u001b[38;5;241m.\u001b[39madd_matrix_w_vector(QKV\u001b[38;5;241m.\u001b[39ma_gpu,\n\u001b[1;32m     27\u001b[0m                     b\u001b[38;5;241m.\u001b[39ma_gpu,\n\u001b[1;32m     28\u001b[0m                     np\u001b[38;5;241m.\u001b[39mint32(QKV\u001b[38;5;241m.\u001b[39mnum_rows),\n\u001b[1;32m     29\u001b[0m                     np\u001b[38;5;241m.\u001b[39mint32(QKV\u001b[38;5;241m.\u001b[39mnum_cols),\n\u001b[1;32m     30\u001b[0m                     QKV_b\u001b[38;5;241m.\u001b[39ma_gpu,\n\u001b[1;32m     31\u001b[0m                     block\u001b[38;5;241m=\u001b[39m(QKV\u001b[38;5;241m.\u001b[39mnum_cols, QKV\u001b[38;5;241m.\u001b[39mnum_rows, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQKV\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/Documents/Transformer/matrix.py:79\u001b[0m, in \u001b[0;36mMatrix.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Documents/Transformer/matrix.py:40\u001b[0m, in \u001b[0;36mMatrix.printed.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 40\u001b[0m   val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodified_since_last_print \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_unstriding \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m/Documents/Transformer/matrix.py:71\u001b[0m, in \u001b[0;36mMatrix.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m       kl\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_gpu,\n\u001b[1;32m     63\u001b[0m              np\u001b[38;5;241m.\u001b[39mint32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_idx),\n\u001b[1;32m     64\u001b[0m              np\u001b[38;5;241m.\u001b[39mint32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m              output_gpu,\n\u001b[1;32m     68\u001b[0m              block\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cols, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     69\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_gpu \u001b[38;5;241m=\u001b[39m output_gpu\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_d_to_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_host \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_host\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cols)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/Documents/Transformer/matrix.py:231\u001b[0m, in \u001b[0;36mMatrix.copy_d_to_h\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPre-allocated on host...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 231\u001b[0m \u001b[43mkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemcpy_dtoh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_host \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_host\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cols)\n",
            "\u001b[0;31mLogicError\u001b[0m: cuMemcpyDtoH failed: invalid argument"
          ]
        }
      ],
      "source": [
        "# Embedding matrix = [vocab_size, token_dims]\n",
        "# Technically you can make swap the dimensions of this and it will still work\n",
        "# One way requires a transpose, the other doesn't\n",
        "# Weights: [3 * token_dims, token_dims]\n",
        "\n",
        "#TODO: Make a weight transpose instead for learnings... Even though a bit less efficient...\n",
        "# weights = Matrix(3 * embeddings.num_cols, embeddings.num_cols, np.float32, gpu=True)\n",
        "# weights.alloc_on_gpu()\n",
        "weights_t = Matrix(embeddings_w_pos.num_cols, 3 * embeddings_w_pos.num_cols, np.float32, gpu=True)\n",
        "weights_t.alloc_on_gpu()\n",
        "weights_scale = kl.xavier_uniform(weights_t.num_rows, weights_t.num_cols)\n",
        "weights_t.init_uniform_rand(weights_scale)\n",
        "\n",
        "# QKV matrix = [vocab_size, 3 * token_dims]\n",
        "QKV = embeddings_w_pos * weights_t\n",
        "\n",
        "bias_scale = kl.xavier_uniform(QKV.num_cols, 1)\n",
        "\n",
        "b = Matrix(QKV.num_cols, 1, np.float32, gpu=True)\n",
        "b.alloc_on_gpu()\n",
        "b.init_uniform_rand(bias_scale)\n",
        "\n",
        "QKV_b = Matrix(QKV.num_rows, QKV.num_cols, np.float32, gpu=True)\n",
        "QKV_b.alloc_on_gpu()\n",
        "\n",
        "kl.add_matrix_w_vector(QKV.a_gpu,\n",
        "                    b.a_gpu,\n",
        "                    np.int32(QKV.num_rows),\n",
        "                    np.int32(QKV.num_cols),\n",
        "                    QKV_b.a_gpu,\n",
        "                    block=(QKV.num_cols, QKV.num_rows, 1))\n",
        "\n",
        "print(f\"{QKV=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lazily allocating on host!\n",
            "Allocating 12 <class 'numpy.float32'> onto the host...\n",
            "Lazily allocating on host!\n",
            "Allocating 48 <class 'numpy.float32'> onto the host...\n",
            "Lazily allocating on host!\n",
            "Allocating 48 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "QKV_b=[[ 0.5858841  -0.07326937 -1.5218644   0.3128903  -0.14234573  1.1255311\n",
            "  -0.9151788  -1.2557507  -1.4943337   0.22304492  0.5137239  -0.76285994]\n",
            " [ 0.7072086   1.4134607  -1.5254618   1.6387186   1.2436802   0.12450986\n",
            "   0.19428861 -1.266494   -0.5250175   0.16229238  0.92345756 -0.8253772 ]\n",
            " [ 0.10867439  1.2466791  -1.4293613   1.5948985   0.989475   -0.02572972\n",
            "   0.2553985  -0.845855   -0.43513     0.07947831  0.9863903  -0.64482915]\n",
            " [-0.16197938  1.1431178  -1.4462063   1.5293294   0.98203295 -0.33542147\n",
            "   0.57010895 -0.66561127 -0.07674776 -0.0632465   1.0501893  -0.44635892]]\n",
            "QKV_b_np=array([[ 0.58588406, -0.07326933, -1.52186439,  0.3128903 , -0.14234572,\n",
            "         1.12553115, -0.91517874, -1.2557506 , -1.49433365,  0.22304491,\n",
            "         0.51372392, -0.7628599 ],\n",
            "       [ 0.70720858,  1.4134607 , -1.52546181,  1.63871851,  1.24368024,\n",
            "         0.12450988,  0.19428861, -1.26649397, -0.52501754,  0.16229238,\n",
            "         0.9234576 , -0.82537725],\n",
            "       [ 0.10867436,  1.24667903, -1.4293614 ,  1.59489841,  0.98947497,\n",
            "        -0.02572972,  0.25539853, -0.84585492, -0.43512996,  0.0794783 ,\n",
            "         0.98639025, -0.64482916],\n",
            "       [-0.16197938,  1.14311773, -1.44620626,  1.52932941,  0.98203293,\n",
            "        -0.33542146,  0.57010891, -0.66561129, -0.07674777, -0.06324649,\n",
            "         1.05018927, -0.44635892]])\n",
            "QKV_b not similar!\n"
          ]
        }
      ],
      "source": [
        "b.copy_d_to_h()\n",
        "b_np = b.a_host.copy()\n",
        "b_np = b_np.T\n",
        "\n",
        "# weights_t_np = np.random.uniform(low=-weights_scale, high=weights_scale, size=(weights_t.num_rows, weights_t.num_cols))\n",
        "weights_t.copy_d_to_h()\n",
        "weights_t_np = weights_t.a_host.copy()\n",
        "\n",
        "QKV_np = embeddings_w_pos_np @ weights_t_np\n",
        "QKV_b_np = QKV_np + b_np\n",
        "\n",
        "if (not QKV_b.compare(QKV_b_np)):\n",
        "  print(\"QKV_b are similar!\")\n",
        "else:\n",
        "  print(f\"{QKV_b=}\")\n",
        "  print(f\"{QKV_b_np=}\")\n",
        "  print(\"QKV_b not similar!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Gathering the elements for this matrix....\n",
            "Pre-allocated on host...\n",
            "small_1=[[ 0.  1.  2.  3.]\n",
            " [12. 13. 14. 15.]\n",
            " [24. 25. 26. 27.]\n",
            " [36. 37. 38. 39.]]\n",
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Gathering the elements for this matrix....\n",
            "Pre-allocated on host...\n",
            "small_2=[[ 0.  1.  2.  3.]\n",
            " [12. 13. 14. 15.]\n",
            " [24. 25. 26. 27.]\n",
            " [36. 37. 38. 39.]]\n",
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Gathering the elements for this matrix....\n",
            "Pre-allocated on host...\n",
            "small_3=[[ 0.  1.  2.  3.]\n",
            " [12. 13. 14. 15.]\n",
            " [24. 25. 26. 27.]\n",
            " [36. 37. 38. 39.]]\n"
          ]
        }
      ],
      "source": [
        "test_big_matrix = Matrix(4, 12, np.float32, gpu=True)\n",
        "test_big_matrix.alloc_on_gpu()\n",
        "test_big_matrix.init_incremental()\n",
        "\n",
        "small_1 = Matrix(4, 4, np.float32, gpu=True)\n",
        "small_1.set_gpu_matrix(test_big_matrix.a_gpu, stride=test_big_matrix.num_cols, start_idx=(0 * test_big_matrix.num_cols) / 3)\n",
        "\n",
        "small_2 = Matrix(4, 4, np.float32, gpu=True)\n",
        "small_2.set_gpu_matrix(test_big_matrix.a_gpu, stride=test_big_matrix.num_cols, start_idx=(0 * test_big_matrix.num_cols) / 3)\n",
        "\n",
        "small_3 = Matrix(4, 4, np.float32, gpu=True)\n",
        "small_3.set_gpu_matrix(test_big_matrix.a_gpu, stride=test_big_matrix.num_cols, start_idx=(0 * test_big_matrix.num_cols) / 3)\n",
        "\n",
        "print(f\"{small_1=}\")\n",
        "print(f\"{small_2=}\")\n",
        "print(f\"{small_3=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "Q = Matrix(QKV_b.num_rows, QKV_b.num_cols / 3, np.float32, gpu=True)\n",
        "Q.set_gpu_matrix(QKV_b.a_gpu, stride=QKV_b.num_cols, start_idx=(0 * QKV_b.num_cols) / 3)\n",
        "\n",
        "K = Matrix(QKV_b.num_rows, QKV_b.num_cols / 3, np.float32, gpu=True)\n",
        "K.set_gpu_matrix(QKV_b.a_gpu, stride=QKV_b.num_cols, start_idx=(1 * QKV_b.num_cols) / 3)\n",
        "\n",
        "V = Matrix(QKV_b.num_rows, QKV_b.num_cols / 3, np.float32, gpu=True)\n",
        "V.set_gpu_matrix(QKV_b.a_gpu, stride=QKV_b.num_cols, start_idx=(2 * QKV_b.num_cols) / 3)\n",
        "\n",
        "score_scaled = (Q * K.transpose()) / math.sqrt(Q.num_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lazily allocating on host!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Gathering the elements for this matrix....\n",
            "Pre-allocated on host...\n",
            "Q=[[ 0.5858841  -0.07326937 -1.5218644   0.3128903 ]\n",
            " [ 0.7072086   1.4134607  -1.5254618   1.6387186 ]\n",
            " [ 0.10867439  1.2466791  -1.4293613   1.5948985 ]\n",
            " [-0.16197938  1.1431178  -1.4462063   1.5293294 ]]\n",
            "Q_np=array([[ 0.58588406, -0.07326933, -1.52186439,  0.3128903 ],\n",
            "       [ 0.70720858,  1.4134607 , -1.52546181,  1.63871851],\n",
            "       [ 0.10867436,  1.24667903, -1.4293614 ,  1.59489841],\n",
            "       [-0.16197938,  1.14311773, -1.44620626,  1.52932941]])\n",
            "Q is not similar\n",
            "Lazily allocating on host!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Gathering the elements for this matrix....\n",
            "Pre-allocated on host...\n",
            "K=[[-0.14234573  1.1255311  -0.9151788  -1.2557507 ]\n",
            " [ 1.2436802   0.12450986  0.19428861 -1.266494  ]\n",
            " [ 0.989475   -0.02572972  0.2553985  -0.845855  ]\n",
            " [ 0.98203295 -0.33542147  0.57010895 -0.66561127]]\n",
            "K_np=array([[-0.14234572,  1.12553115, -0.91517874, -1.2557506 ],\n",
            "       [ 1.24368024,  0.12450988,  0.19428861, -1.26649397],\n",
            "       [ 0.98947497, -0.02572972,  0.25539853, -0.84585492],\n",
            "       [ 0.98203293, -0.33542146,  0.57010891, -0.66561129]])\n",
            "K is not similar\n",
            "Lazily allocating on host!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Gathering the elements for this matrix....\n",
            "Pre-allocated on host...\n",
            "V=[[-1.4943337   0.22304492  0.5137239  -0.76285994]\n",
            " [-0.5250175   0.16229238  0.92345756 -0.8253772 ]\n",
            " [-0.43513     0.07947831  0.9863903  -0.64482915]\n",
            " [-0.07674776 -0.0632465   1.0501893  -0.44635892]]\n",
            "V_np=array([[-1.49433365,  0.22304491,  0.51372392, -0.7628599 ],\n",
            "       [-0.52501754,  0.16229238,  0.9234576 , -0.82537725],\n",
            "       [-0.43512996,  0.0794783 ,  0.98639025, -0.64482916],\n",
            "       [-0.07674777, -0.06324649,  1.05018927, -0.44635892]])\n",
            "V is not similar\n"
          ]
        }
      ],
      "source": [
        "split_dim = int(QKV_b_np.shape[1] / 3)\n",
        "\n",
        "Q_np = QKV_b_np[:, : split_dim]\n",
        "K_np = QKV_b_np[:, split_dim : 2 * split_dim]\n",
        "V_np = QKV_b_np[:, 2 * split_dim :]\n",
        "\n",
        "if (Q.compare(Q_np)):\n",
        "  print(\"Q is similar!\")\n",
        "else:\n",
        "  print(f\"{Q=}\")\n",
        "  print(f\"{Q_np=}\")\n",
        "  print(\"Q is not similar\")\n",
        "\n",
        "if (K.compare(K_np)):\n",
        "  print(\"K is similar!\")\n",
        "else:\n",
        "  print(f\"{K=}\")\n",
        "  print(f\"{K_np=}\")\n",
        "  print(\"K is not similar\")\n",
        "\n",
        "if (V.compare(V_np)):\n",
        "  print(\"V is similar!\")\n",
        "else:\n",
        "  print(f\"{V=}\")\n",
        "  print(f\"{V_np=}\")\n",
        "  print(\"V is not similar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "score_scaled=[[ 0.41700038  0.01378758 -0.03586942 -0.2379791 ]\n",
            " [ 0.41423714 -0.65813804 -0.55616087 -0.870016  ]\n",
            " [ 0.34651658 -1.003629   -0.81932783 -1.0939575 ]\n",
            " [ 0.3563763  -1.1384946  -0.9263183  -1.1924647 ]]\n",
            "score_scaled_np=array([[ 0.41700038,  0.01378754, -0.03586946, -0.23797911],\n",
            "       [ 0.4142372 , -0.6581379 , -0.55616079, -0.87001593],\n",
            "       [ 0.34651665, -1.00362892, -0.81932781, -1.09395758],\n",
            "       [ 0.35637629, -1.13849449, -0.9263182 , -1.19246465]])\n"
          ]
        }
      ],
      "source": [
        "score_scaled = (Q * K.transpose()) / math.sqrt(Q.num_cols)\n",
        "score_scaled_np = (Q_np @ K_np.T) / math.sqrt(Q.num_cols)\n",
        "\n",
        "print(f\"{score_scaled=}\")\n",
        "print(f\"{score_scaled_np=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "test_a=[[ 0.  1.  2.  3.]\n",
            " [ 4.  5.  6.  7.]\n",
            " [ 8.  9. 10. 11.]\n",
            " [12. 13. 14. 15.]]\n",
            "Allocating 4 on host to print this matrix!\n",
            "Allocating 4 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "Before: test_output=[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            "Returning the cached matrix value!\n",
            "After: test_output=[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n"
          ]
        }
      ],
      "source": [
        "test_a = Matrix(4,4,np.float32,gpu=True)\n",
        "test_a.alloc_on_gpu()\n",
        "test_a.init_incremental()\n",
        "\n",
        "test_output = Matrix(4,1,np.float32,gpu=True)\n",
        "test_output.alloc_on_gpu()\n",
        "test_output.init_incremental()\n",
        "\n",
        "print(f\"{test_a=}\")\n",
        "print(f\"Before: {test_output=}\")\n",
        "\n",
        "kl.matrix_row_wise_add(test_a.a_gpu,\n",
        "        np.int32(test_a.num_rows),\n",
        "        np.int32(test_a.num_cols),\n",
        "        test_output.a_gpu,\n",
        "        block=(test_a.num_cols,test_a.num_rows,1),\n",
        "        shared=test_a.num_elements() * test_a.dtype().nbytes)\n",
        "\n",
        "print(f\"After: {test_output=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "score_softmaxed=[[0.35417998 0.23665237 0.22518794 0.18397976]\n",
            " [0.50050443 0.17127    0.18965724 0.1385683 ]\n",
            " [0.5531961  0.14338982 0.17240874 0.13100538]\n",
            " [0.5834099  0.13084576 0.16177322 0.12397116]]\n"
          ]
        }
      ],
      "source": [
        "score_softmaxed = Matrix(score_scaled.num_rows, score_scaled.num_cols, np.float32, gpu=True)\n",
        "score_softmaxed.alloc_on_gpu()\n",
        "score_softmaxed.init_incremental()\n",
        "\n",
        "matrix_bytes = score_scaled.num_elements() * score_scaled.dtype().nbytes\n",
        "shared_mem_bytes = int((3 * matrix_bytes) / 2)\n",
        "\n",
        "kl.fused_softmax(score_scaled.a_gpu,\n",
        "              np.int32(score_scaled.num_rows),\n",
        "              np.int32(score_scaled.num_cols),\n",
        "              score_softmaxed.a_gpu,\n",
        "              block=(score_scaled.num_cols, score_scaled.num_rows, 1),\n",
        "              shared=shared_mem_bytes)\n",
        "\n",
        "print(f\"{score_softmaxed=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "score_softmaxed_np=array([[0.34433264, 0.23070931, 0.2349146 , 0.19004345],\n",
            "       [0.48925908, 0.16788517, 0.19893495, 0.1439208 ],\n",
            "       [0.54172503, 0.14080503, 0.18116312, 0.13630682],\n",
            "       [0.57201307, 0.12864469, 0.17019606, 0.12914618]])\n"
          ]
        }
      ],
      "source": [
        "score_scaled_row_sum_np = np.sum(score_scaled_np, axis=1)\n",
        "score_scaled_row_max_np = np.max(score_scaled_np, axis=1)\n",
        "\n",
        "# softmax = exp(score_scaled - score_scaled_row_max) / score_scaled_row_sum_np\n",
        "score_shifted_np = np.exp(score_scaled_np - score_scaled_row_max_np)\n",
        "score_shifted_sum_np = np.sum(score_shifted_np, axis=1, keepdims=True)\n",
        "score_softmaxed_np = score_shifted_np / score_shifted_sum_np\n",
        "print(f\"{score_softmaxed_np=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Returning the cached matrix value!\n",
            "V=[[-1.4943337   0.22304492  0.5137239  -0.76285994]\n",
            " [-0.5250175   0.16229238  0.92345756 -0.8253772 ]\n",
            " [-0.43513     0.07947831  0.9863903  -0.64482915]\n",
            " [-0.07674776 -0.0632465   1.0501893  -0.44635892]]\n",
            "V_np=array([[-1.49433365,  0.22304491,  0.51372392, -0.7628599 ],\n",
            "       [-0.52501754,  0.16229238,  0.9234576 , -0.82537725],\n",
            "       [-0.43512996,  0.0794783 ,  0.98639025, -0.64482916],\n",
            "       [-0.07674777, -0.06324649,  1.05018927, -0.44635892]])\n",
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "attention=[[-0.7656158   0.12366641  0.81582594 -0.69284594]\n",
            " [-0.93100077  0.14574048  0.74788064 -0.70732486]\n",
            " [-0.98701626  0.15207577  0.7242472  -0.7100114 ]\n",
            " [-1.0204123   0.15637861  0.7103068  -0.7127089 ]]\n",
            "attention_np=array([[-0.75247807,  0.12089504,  0.82124124, -0.68940715],\n",
            "       [-0.91686712,  0.14308176,  0.7537505 , -0.70432412],\n",
            "       [-0.97273379,  0.1494582 ,  0.73017007, -0.7071386 ],\n",
            "       [-1.00628818,  0.15382151,  0.71616238, -0.70993916]])\n"
          ]
        }
      ],
      "source": [
        "# print(f\"{score_softmaxed=}\")\n",
        "# print(f\"{score_softmaxed_np=}\")\n",
        "\n",
        "print(f\"{V=}\")\n",
        "print(f\"{V_np=}\")\n",
        "\n",
        "attention = score_softmaxed * V\n",
        "attention_np = score_softmaxed_np @ V_np\n",
        "\n",
        "print(f\"{attention=}\")\n",
        "print(f\"{attention_np=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Returning the cached matrix value!\n",
            "attention=[[-0.7656158   0.12366641  0.81582594 -0.69284594]\n",
            " [-0.93100077  0.14574048  0.74788064 -0.70732486]\n",
            " [-0.98701626  0.15207577  0.7242472  -0.7100114 ]\n",
            " [-1.0204123   0.15637861  0.7103068  -0.7127089 ]]\n",
            "Allocating 16 on host to print this matrix!\n",
            "Allocating 16 <class 'numpy.float32'> onto the host...\n",
            "Pre-allocated on host...\n",
            "[[-0.3495437   1.8528491   0.01738602  1.1194216 ]\n",
            " [ 0.64683527  0.5931184   1.0476859  -0.3829947 ]\n",
            " [-0.12930167 -0.24121895  1.2225952  -0.06568772]\n",
            " [-0.50861675 -1.0786959   1.0545287  -0.07339954]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"{attention=}\")\n",
        "add = embeddings_w_pos + attention\n",
        "print(f\"{add}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNOSc2A+76qSEFerkYh9pBl",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
