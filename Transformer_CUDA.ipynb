{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sc4gxOF5TVJP"
      },
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "from kernel_lib import *\n",
        "\n",
        "# import importlib\n",
        "# importlib.reload(kernel_lib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dFsy5cCSbJK-"
      },
      "outputs": [],
      "source": [
        "kernel_code = \"\"\"\n",
        "#include <curand_kernel.h>\n",
        "#include <math.h>\n",
        "\n",
        "extern \"C\" __global__ void generate_random_numbers(float* numbers, int seed, int N) {\n",
        "  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  if (idx < N) {\n",
        "    curandState state;\n",
        "    curand_init(seed, idx, 0, &state);\n",
        "    numbers[idx] = curand_uniform(&state);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\" __global__ void debug_func(void) {\n",
        "  printf(\"Debug print %f\\\\n\", powf(2, 1));\n",
        "}\n",
        "\n",
        "// Init array with some provided value\n",
        "extern \"C\" __global__ void init_array_w_val(float* arr, int val, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "    arr[idx] = val;\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\" __global__ void calc_positional_encoding(float* pos_enc, int num_rows, int num_cols) {\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < num_rows && col < num_cols) {\n",
        "    int idx = row * num_cols + col;\n",
        "    \n",
        "    int token_idx = row;\n",
        "    int current_dim = col;\n",
        "    int token_dims = num_cols;\n",
        "\n",
        "    pos_enc[idx] = (current_dim & 1) ?\n",
        "                    sinf(token_idx) / powf(10000, (2 * current_dim) / token_dims) :\n",
        "                    cosf(token_idx) / powf(10000, (2 * current_dim) / token_dims);\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,  # This is important!\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "debug_func = mod.get_function(\"debug_func\")\n",
        "init_array_w_val = mod.get_function(\"init_array_w_val\")\n",
        "gen_pos_encodings = mod.get_function(\"calc_positional_encoding\")\n",
        "generate_random_numbers = mod.get_function(\"generate_random_numbers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [\"This\", \"is\", \"a\", \"sentence\"]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "pos_enc_seq_len = 10\n",
        "token_dims = vocab_size\n",
        "\n",
        "pos_encodings_num_elements = pos_enc_seq_len * token_dims\n",
        "pos_encodings_size_bytes = pos_encodings_num_elements * np.float32().nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encoding=[[ 1.0000e+00  0.0000e+00  1.0000e-04  0.0000e+00]\n",
            " [ 5.4030e-01  8.4147e-01  5.4030e-05  8.4147e-05]\n",
            " [-4.1615e-01  9.0930e-01 -4.1615e-05  9.0930e-05]\n",
            " [-9.8999e-01  1.4112e-01 -9.8999e-05  1.4112e-05]\n",
            " [-6.5364e-01 -7.5680e-01 -6.5364e-05 -7.5680e-05]\n",
            " [ 2.8366e-01 -9.5892e-01  2.8366e-05 -9.5892e-05]\n",
            " [ 9.6017e-01 -2.7942e-01  9.6017e-05 -2.7942e-05]\n",
            " [ 7.5390e-01  6.5699e-01  7.5390e-05  6.5699e-05]\n",
            " [-1.4550e-01  9.8936e-01 -1.4550e-05  9.8936e-05]\n",
            " [-9.1113e-01  4.1212e-01 -9.1113e-05  4.1212e-05]]\n"
          ]
        }
      ],
      "source": [
        "pos_encodings_gpu = cuda.mem_alloc(pos_encodings_size_bytes)\n",
        "init_array_w_val(pos_encodings_gpu, np.int32(123), np.int32(pos_encodings_num_elements), block=(pos_encodings_num_elements,1,1))\n",
        "gen_pos_encodings(pos_encodings_gpu, np.int32(pos_enc_seq_len), np.int32(token_dims), block=(token_dims, pos_enc_seq_len, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(pos_encodings_gpu,\n",
        "                \"pos_encoding\",\n",
        "                pos_encodings_num_elements,\n",
        "                shape=[pos_enc_seq_len, token_dims],\n",
        "                verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = \"This is a sentence\"\n",
        "sentence_toks = [0, 1, 2, 3] # Straight forward\n",
        "word2tok = {\"This\" : 0, \"is\" : 1, \"a\" : 2, \"sentence\" : 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding matrix shape : (token, vector dimensions)\n",
        "embedding_num_elements = vocab_size * token_dims\n",
        "embedding_size_bytes = embedding_num_elements * np.float32().nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DNS1PI19Tvas"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding matrix=[[0.7402 0.921  0.039  0.969 ]\n",
            " [0.9251 0.4464 0.6673 0.1099]\n",
            " [0.4702 0.5132 0.7762 0.2948]\n",
            " [0.714  0.3585 0.6814 0.292 ]]\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix_gpu = cuda.mem_alloc(embedding_size_bytes)\n",
        "# init_array(embedding_matrix_gpu, np.int32(embedding_num_elements), block=(embedding_num_elements, 1, 1))\n",
        "generate_random_numbers(embedding_matrix_gpu, np.int32(0), np.int32(embedding_num_elements), block=(embedding_num_elements, 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(embedding_matrix_gpu,\n",
        "                \"embedding matrix\",\n",
        "                embedding_num_elements,\n",
        "                shape=[vocab_size, token_dims])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "kernel_code = \"\"\"\n",
        "// Assumes embedding matrix has been sized such that Dim(embedding_matrix) < Dim(pos_enc)\n",
        "extern \"C\" __global__ void add_pos_enc_and_embed(float* embedding_matrix, float* pos_enc, float* output, int N) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "    output[idx] = embedding_matrix[idx] + pos_enc[idx];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "mod = SourceModule(kernel_code,\n",
        "                   no_extern_c=True,\n",
        "                   options=[\"-std=c++11\",\n",
        "                           \"-Xcompiler\",\n",
        "                           \"-fPIC\"])\n",
        "\n",
        "add_pos_enc_and_embed = mod.get_function(\"add_pos_enc_and_embed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encoded_emb=[[ 1.7402  0.921   0.0391  0.969 ]\n",
            " [ 1.4654  1.2878  0.6674  0.11  ]\n",
            " [ 0.0541  1.4225  0.7761  0.2949]\n",
            " [-0.276   0.4996  0.6813  0.292 ]]\n"
          ]
        }
      ],
      "source": [
        "pos_encoded_emb_gpu = cuda.mem_alloc(embedding_size_bytes)\n",
        "add_pos_enc_and_embed(embedding_matrix_gpu,\n",
        "                      pos_encodings_gpu,\n",
        "                      pos_encoded_emb_gpu,\n",
        "                      np.int32(embedding_num_elements),\n",
        "                      block=(embedding_num_elements, 1, 1))\n",
        "cuda.Context.synchronize()\n",
        "\n",
        "print_gpu_array(pos_encoded_emb_gpu,\n",
        "                \"pos_encoded_emb\",\n",
        "                embedding_num_elements,\n",
        "                shape=[vocab_size, token_dims])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LUcUeVvLUTuz"
      },
      "outputs": [],
      "source": [
        "# Take the input sentence\n",
        "# convert to tokens (idices)\n",
        "# TODO(MASAAD): Do this later, assume done for now\n",
        "# Use sentence_toks\n",
        "\n",
        "# use tokens as lookup into embedding matrix\n",
        "# Add embedding element + positional encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_layer_code = \"\"\"\n",
        "// extern \"C\" __device__ void \n",
        "\n",
        "// Inputs x is a matrix and w is a vector\n",
        "// Dereference the vector in x and vector multiply by w\n",
        "extern \"C\" __global__ void linear_layer(float* x, float* w, int num_rows, int num_cols) {\n",
        "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (idx < N) {\n",
        "\n",
        "  }\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basically a Tensor class definition...\n",
        "class Matrix():\n",
        "  def __init__(self, num_rows, num_cols, dtype, gpu=False):\n",
        "    self.num_rows = num_rows\n",
        "    self.num_cols = num_cols\n",
        "    self.dtype = dtype\n",
        "    self.gpu = gpu\n",
        "    self.allocated_on_gpu = False\n",
        "    self.allocated_on_host = False\n",
        "    self.a_gpu = None\n",
        "    self.a_host = None\n",
        "    self.stride = 1\n",
        "    self.start_index = 0\n",
        "    self.shape = [self.num_rows, self.num_cols]\n",
        "    # __rmul__ = __mul__\n",
        "\n",
        "  def __str__(self):\n",
        "    if (self.allocated_on_gpu):\n",
        "      a_host = np.empty(self.num_rows * self.num_cols, np.float32)\n",
        "      cuda.memcpy_dtoh(a_host, self.a_gpu)\n",
        "      # np.set_printoptions(threshold=np.inf, precision=4, linewidth=120)\n",
        "      a_host = a_host.reshape(self.num_rows, self.num_cols)\n",
        "      return f\"{a_host}\"\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if (isinstance(other, Matrix)):\n",
        "      if (self.allocated_on_gpu and other.allocated_on_host or\n",
        "          self.allocated_on_host and other.allocated_on_gpu):\n",
        "        raise MemoryError(\"Gpus\")\n",
        "\n",
        "      if (self.allocated_on_gpu and other.allocated_on_gpu):\n",
        "        max_num_rows = max(self.num_rows, other.num_rows)\n",
        "        max_num_cols = max(self.num_cols, other.num_cols)\n",
        "\n",
        "        output = Matrix(self.num_rows, other.num_cols, self.dtype, gpu=True)\n",
        "        output_gpu = cuda.mem_alloc(self.num_rows * self.num_cols * self.dtype().nbytes)\n",
        "        output.set_gpu_matrix(output_gpu)\n",
        "\n",
        "        regular_matmul(self.a_gpu, other.a_gpu,\n",
        "                       np.int32(self.num_rows),\n",
        "                       np.int32(other.num_cols),\n",
        "                       np.int32(self.num_cols),\n",
        "                       output_gpu, block=(max_num_cols, max_num_rows, 1))\n",
        "\n",
        "        return output\n",
        "\n",
        "      elif (self.allocated_on_host and other.allocated_on_host):\n",
        "        raise ValueError(\"Not implemented\")\n",
        "      else:\n",
        "        raise MemoryError(\"ERROR: Not sure if this is possible...\")\n",
        "    else:\n",
        "      raise ValueError(\"Not implemented\")\n",
        "\n",
        "  def __div__(self, other):\n",
        "    if (isinstance(other, int)):\n",
        "      if (self.allocated_on_gpu):\n",
        "        output = Matrix(self.num_rows, other.num_cols, self.dtype, gpu=True)\n",
        "        output_gpu = cuda.mem_alloc(self.num_rows * self.num_cols * self.dtype().nbytes)\n",
        "        output.set_gpu_matrix(output_gpu)\n",
        "\n",
        "        scalar_divide(self.a_gpu,\n",
        "                      other,\n",
        "                      np.int32(self.num_elements()),\n",
        "                      output_gpu,\n",
        "                      block=(max_num_cols, max_num_rows, 1))\n",
        "\n",
        "        return output\n",
        "      elif (self.allocated_on_host):\n",
        "        pass\n",
        "\n",
        "    if (isinstance(other, Matrix)):\n",
        "      if (self.allocated_on_gpu and other.allocated_on_host or\n",
        "          self.allocated_on_host and other.allocated_on_gpu):\n",
        "        raise MemoryError(\"Gpus\")\n",
        "\n",
        "      if (self.allocated_on_gpu and other.allocated_on_gpu):\n",
        "        max_num_rows = max(self.num_rows, other.num_rows)\n",
        "        max_num_cols = max(self.num_cols, other.num_cols)\n",
        "\n",
        "        output = Matrix(self.num_rows, other.num_cols, self.dtype, gpu=True)\n",
        "        output_gpu = cuda.mem_alloc(self.num_rows * self.num_cols * self.dtype().nbytes)\n",
        "        output.set_gpu_matrix(output_gpu)\n",
        "\n",
        "        regular_matmul(self.a_gpu, other.a_gpu,\n",
        "                       np.int32(self.num_rows),\n",
        "                       np.int32(other.num_cols),\n",
        "                       np.int32(self.num_cols),\n",
        "                       output_gpu, block=(max_num_cols, max_num_rows, 1))\n",
        "\n",
        "        return output\n",
        "\n",
        "      elif (self.allocated_on_host and other.allocated_on_host):\n",
        "        raise ValueError(\"Not implemented\")\n",
        "      else:\n",
        "        raise MemoryError(\"ERROR: Not sure if this is possible...\")\n",
        "    else:\n",
        "      raise ValueError(\"Not implemented\")\n",
        "\n",
        "  def num_elements(self):\n",
        "    return self.num_rows * self.num_cols\n",
        "\n",
        "  # Here's a thought experiment, is stride really the most\n",
        "  # optimized storage strategy?...\n",
        "  def set_gpu_matrix(self, a_gpu, stride=1, start_idx=0):\n",
        "    self.allocated_on_gpu = True\n",
        "    self.a_gpu = a_gpu\n",
        "    self.stride = stride\n",
        "    self.start_idx = start_idx\n",
        "\n",
        "  def set_host_matrix(self, a_host, stride=1, start_idx=0):\n",
        "    self.allocated_on_host = True\n",
        "    self.a_host = a_host\n",
        "    self.stride = stride\n",
        "    self.start_idx = start_idx\n",
        "\n",
        "  def alloc_on_gpu(self):\n",
        "    if not self.allocated_on_gpu:\n",
        "      self.allocated_on_gpu = True\n",
        "      self.a_gpu = cuda.mem_alloc(self.num_elements() * self.dtype().nbytes)\n",
        "    else:\n",
        "      print(\"ERROR: Already allocated on GPU\")\n",
        "\n",
        "  def alloc_on_host(self):\n",
        "    if not self.allocated_on_gpu:\n",
        "      self.allocated_on_host = True\n",
        "      self.a_host = np.empty(self.num_elements, self.dtype)\n",
        "    else:\n",
        "      print(\"ERROR: Already allocated on GPU\")\n",
        "\n",
        "  def copy_d_to_h(self):\n",
        "    # Assumes D already allocated\n",
        "    if not self.allocated_on_host:\n",
        "      self.alloc_on_host()\n",
        "    cuda.memcpy_dtoh(self.a_host, self.a_gpu)\n",
        "\n",
        "  def copy_h_to_d(self):\n",
        "    # Assumes H already allocated\n",
        "    if not self.allocated_on_gpu:\n",
        "      self.alloc_on_gpu()\n",
        "    cuda.memcpy_htod(self.a_gpu, self.a_host)\n",
        "  \n",
        "  def init_ones(self):\n",
        "    if (self.allocated_on_gpu):\n",
        "      init_array_w_val(self.a_gpu, np.int32(1), np.int32(self.num_elements()), block=(self.num_elements(),1,1))\n",
        "    elif (self.allocated_on_host):\n",
        "      raise MemoryError(\"Not implemented\")\n",
        "  \n",
        "  def init_incremental(self):\n",
        "    if (self.allocated_on_gpu):\n",
        "      init_array(self.a_gpu, np.int32(self.num_elements()), block=(self.num_elements(),1,1))\n",
        "    elif (self.allocated_on_host):\n",
        "      raise MemoryError(\"Not implemented\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 56.  62.  68.  74.]\n",
            " [152. 174. 196. 218.]\n",
            " [248. 286. 324. 362.]\n",
            " [344. 398. 452. 506.]]\n"
          ]
        }
      ],
      "source": [
        "test_a = Matrix(4,4,np.float32,gpu=True)\n",
        "test_a.alloc_on_gpu()\n",
        "test_a.init_incremental()\n",
        "\n",
        "test_b = Matrix(4,4,np.float32,gpu=True)\n",
        "test_b.alloc_on_gpu()\n",
        "test_b.init_incremental()\n",
        "\n",
        "test_c = test_a * test_b\n",
        "print(test_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[369202.75  423964.2   478181.53  533325.6  ]\n",
            " [115491.266 132623.73  149586.53  166838.36 ]\n",
            " [189715.78  217855.4   245715.78  274051.4  ]\n",
            " [210325.3   241523.2   272411.6   303827.78 ]]\n"
          ]
        }
      ],
      "source": [
        "# Embedding matrix = [vocab_size, token_dims]\n",
        "# Technically you can make swap the dimensions of this and it will still work\n",
        "# One way requires a transpose, the other doesn't\n",
        "# Weights: [3 * token_dims, token_dims]\n",
        "\n",
        "weights_dim = token_dims\n",
        "weights_num_elements = 3 * token_dims * weights_dim\n",
        "weights_size_bytes = weights_num_elements * np.float32().nbytes\n",
        "weights_matrix_gpu = cuda.mem_alloc(weights_size_bytes)\n",
        "\n",
        "# QKV matrix = [vocab_size, 3 * token_dims]\n",
        "qkv_matrix_dim = vocab_size\n",
        "qkv_matrix_num_elements = qkv_matrix_dim * 3 * token_dims\n",
        "qkv_matrix_bytes = qkv_matrix_num_elements * np.float32().nbytes\n",
        "qkv_matrix_gpu = cuda.mem_alloc(qkv_matrix_bytes)\n",
        "\n",
        "regular_matmul(embedding_matrix_gpu, weights_matrix_gpu, np.int32(vocab_size), np.int32(3 * token_dims), np.int32(token_dims), qkv_matrix_gpu, block=(3 * token_dims, vocab_size, 1))\n",
        "\n",
        "bias_vector_dim = 3 * token_dims\n",
        "bias_size_bytes = bias_vector_dim * np.float32().nbytes\n",
        "bias_vector_gpu = cuda.mem_alloc(bias_size_bytes)\n",
        "\n",
        "generate_random_numbers(bias_vector_gpu, np.int32(0), np.int32(bias_vector_dim), block=(bias_vector_dim, 1, 1))\n",
        "\n",
        "qkv_matrix_w_bias_gpu = cuda.mem_alloc(qkv_matrix_bytes)\n",
        "add_matrix_w_vector(qkv_matrix_gpu, bias_vector_gpu, np.int32(qkv_matrix_dim), np.int32(3 * token_dims), qkv_matrix_w_bias_gpu, block=(3 * token_dims, qkv_matrix_dim, 1))\n",
        "\n",
        "qkv_matrix = Matrix(qkv_matrix_dim, 3 * token_dims, np.float32, gpu=True)\n",
        "qkv_matrix.set_gpu_matrix(qkv_matrix_w_bias_gpu)\n",
        "\n",
        "Q = Matrix(qkv_matrix_dim, token_dims, np.float32, gpu=True)\n",
        "Q.set_gpu_matrix(qkv_matrix_w_bias_gpu, stride=3*token_dims, start_idx=0)\n",
        "\n",
        "K = Matrix(qkv_matrix_dim, token_dims, np.float32, gpu=True)\n",
        "K.set_gpu_matrix(qkv_matrix_w_bias_gpu, stride=3*token_dims, start_idx=token_dims)\n",
        "\n",
        "V = Matrix(qkv_matrix_dim, token_dims, np.float32, gpu=True)\n",
        "V.set_gpu_matrix(qkv_matrix_w_bias_gpu, stride=3*token_dims, start_idx=2*token_dims)\n",
        "\n",
        "score = Q * K\n",
        "print(score)\n",
        "\n",
        "# print_gpu_array(qkv_matrix_gpu, \"qkv_matrix_gpu\", qkv_matrix_num_elements, shape=[qkv_matrix_dim, 3 * token_dims])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNOSc2A+76qSEFerkYh9pBl",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
